Feature Engineering
[make your data better suited to the problem at hand]

Temperature: result of a kind of feature engineering
attempt to make the observed data more relevant to what we actually care about?

Do feature engineering to
-improve a model's predictive performance
-reduce computational or data needs
-improve interpretability of the results

transform the features to make their relationship to the target linear. [ex) Parabolic]

Baselines: good practice at the start of the feature engineering process. << Compare new feature result with a baseline

Idea[1]: ratios of the features (if cooking)

----------------------------------------------
Mutual information

To begin with a huge dataset: construct a ranking with a feature utility metric

feature utility metric(mutual information := correlation): measuring associations between a feature and the target
then choose smaller set of the most useful features

Mutual information: can detect any kind of relationship
Correlation: can only detect linear relationship

Pros of mutual information
-easy to use and interpret,
-computationally efficient,
-theoretically well-founded,
-resistant to overfitting, and,
-able to detect any kind of relationship

MI:  knowledge of one quantity reduces uncertainty about the other.
"how many questions you expect the feature to answer about the target."


Log function: increases very slowly.
[0, inf], but >2 -> uncommin

Uncertainty := entropy: "how many yes-or-no questions you would need to describe an occurance of that variable, on average."

[MI can't detect interactions between features. It is a univariate metric.]

- The actual usefulness of a feature depends on the model you use it with. may need to transform the feature first to expose the association.

MI 쓰기 전에 Categorical을 전부 encoding해야 한다.
MI in Scikit-learn:
from sklearn.feature_selection import mutual_info_regression
from sklearn.feature_selection import mutual_info_classif

Data visualization: great addition to feature engineering.
Can help you discover important relationships in your data.

Before deciding a feature is unimportant from its MI score, it's good to investigate any possible interaction effects
-- domain knowledge can offer a lot of guidance here.

pd.melt에 대한 설명
https://koreadatascientist.tistory.com/12

```
mi_scores.head(10)
```
Combining these top features with other related features, especially those you've identified as creating interactions, is a good strategy for coming up with a highly informative set of features to train your model on.

-----------------------------------------------
Creating Features

Once you've identified a set of features with some potential, it's time to start developing them.

Tips on Discovering New Feartures.
- Understand the features. Refer to your dataset's data documentation, if available.
- Research the problem domain to acquire domain knowledge. books and journal articles will often have the best information.
- Study previous work. Solution write-ups from past Kaggle competitions are a great resource.
- [Visualization] can reveal pathologies in the distribution of a feature or complicated relationships that could be simplified.

[idea 1]: Mathematical Transforms (a = b / c)
[idea 2]: log or power transformations
[idea 3]: Count: Aggregate of features describing same thing.
ex) In Traffic Accidents, several features indicating whether some roadway object was near the accident
>> Aggregate these features and create total number of roadway features nearby by using `sum`

ex2) `gt(0).sum()`을 이용해 concrete 배합에 실제로 들어간 component 의 개수만 뽑기.

[idea 4]: Split and Join
ex) '(999) 555-0123' 미국 전화번호에서 (999) area code 추출하기.

ex2) 자동차 제조사와 자동차 종류를 join (audi_sedan)

[idea 5]: Group transformation use `groupby` and `transform`

ex) "average income by state", "frequency encoding" for a categorical feature.

For independence, create grouped feature using only the training set and then join it to the validation set. Can use `merge` and `drop_duplicates`
=> Train의 값만 이용해서 Group 값을 계산하고 이를 Validation에 박아넣는다고 생각하면 될듯.

[[Guidelines on Creating Features.]]	
-When creating Features, keep in mind that your model's own strengths and weeknesses.

- Linear models learn sums and differences naturally, but can't learn anything more complex.
- `Ratios` seem to be difficult for most models to learn. Often lead to some easy performance gains.
- Linear models and neural nets better with normalized features. Neural nets especially need features scaled to values not too far from 0.
- Tree-based models (like random forests and XGBoost) sometimes benefit ,but usually much less so.
- Tree models can learn to approximate almost any combination of features, but when a combination is especially important they can still benefit from having it explicitly created, especially when data is limited.
- Counts are especially helpful for tree models, since these models don't have a natural way of aggregating information across many features at once.

- having the same units (square-feet) makes it easy to combine them in sensible ways, including ratio and sum as unit

- Interaction Between categorical and continuous
Categorical을 dummy로 만든 다음에 서로 곱해준다.

```python
Split elements and get only first element
X_4['MSClass'] = X.MSSubClass.str.split('_', n=1, expand=True)[0]

Group by transformation example
X_5["MedNhbdArea"] = X.groupby('Neighborhood')['GrLivArea'].transform('median')
```

-----------------------------------------------
Clustering With K-Means

Unsupervised learning to learn some property of the data, to represent the structure of the features in a certain way
>> Feature Discovery Example

Clustring = binning, Feature discretization
Binning = group a number of more or less continuous values into a smaller number of "bins"
ex) 나이(int) => 10대, 20대, 30대...
Feature discretization = decomposes each feature into a set of bins, equally distributed in width.

- Clustering also available with 2 features
ex) x = Longtitude, y = Latitude, dot = house price cluster

- "divide and conquer" strategy
can learn simpler chunks one-by-one
instead leraning complicated whole all at once.

Application에 따라 K-means 말고 다른 Algorithm으로 Custring해도 됨.
centroids
Voronoi tessallation

- Algorithm 진행
1. randomly initializing `n_clusters` of centroids.
2. assign points to the nearest cluster centroid
3. move each centroid to minimize the distance to its points
4. repeat 2, 3 until the centroids aren't moving anymore or when reaching `max_iter`.

ICO poor clustring, algorithm repeats 1~4 `n_init` time and returns least total distance between each point and its centroid, the optimal clustering.

may need to increase the `max_iter` for a large number of clusters or `n_init` for a complex dataset.

Ordinarily though the only parameter you'll need to choose yourself is `n_clusters` (k, that is).

- k-means clustering is sensitive to scale, it can be a good idea rescale or normalize data with extreme values.

As a rule of thumb, if the features are already directly comparable (like a test result at different times), then you would not want to rescale.
features that aren't on comparable scales (like height and weight) will usually benefit from rescaling.
Sometimes, the choice won't be clear though. In that case, you should try to use common sense, remembering that features with larger values will be weighted more heavily.
↓
요약
>> Consider the following sets of features. For each, decide whether:
they definitely should be rescaled,
they definitely should not be rescaled, or
either might be reasonable
- Rescaling depends on some domain knowledge about your data and what you're trying to predict.
- Comparing different rescaling schemes through cross-validation can also be helpful.

`Lot Area` and `Living Area` of houses in Ames, Iowa
it would make sense to rescale so that lot area isn't weighted in the clustering out of proportion to its effect on `SalePrice`

`Number of Doors` and `Horsepower` of a 1989 model car
Without rescaling, the number of doors in a car (usually 2 or 4) would have negligible weight compared to its horsepower (usually in the hundreds).

Cluster-Distance Features of k-means
-can measure the distance from a point to all the centroids and return those distances as features. >> least distance로 분류됨.

- `fit_transform` method of `kmeans` instead of `fit_predict`

-----------------------------------------------
Principal Component Analysis (PCA)

clustering: partitioning of the dataset based on proximity
PCA: partitioning of the variation in the data.
- instead of describing the data with the original features, 
describe it with its axes of variation.
ex) X = height, Y = Diamenter일 때
우상향 대각선 = Size, 우하향 대각선 = Shape

- gives you direct access to the correlational structure of your data.

- Technical Note
PCA is typically applied to standardized data.
With standardized data "variation" means "correlation".
With unstandardized data "variation" means "covariance".
All data in this course will be standardized before applying PCA.

abalone = 전복

new features PCA constructs are linear combinations of the original features:
```
df["Size"] = 0.707 * X["Height"] + 0.707 * X["Diameter"]
df["Shape"] = 0.707 * X["Height"] - 0.707 * X["Diameter"]
```
These new features are called the `principal components` of the data. The weights themselves are called `loadings`.

two ways you could use PCA for feature engineering.
1. Descriptive technique to discover features.. Since the components tell you about the variation.

- see what kind of variation is most predictive of your target through MI score.
- That could give you ideas for kinds of features to create.
ex) 'Height' X 'Diameter' if 'Size' is important, 
'Height' / 'Diameter' if Shape is important.
- Could even try clustering on one or more of the high-scoring components.

2. Use the components themselves as features. Because the components expose the variational structure of the data directly.

- redundancy: the inclusion of extra components which are not strictly necessary to functioning, in case of failure in other components. = 불필요한 data

- Multicollinearity: An independent variable is highly correlated with one or more of the other independent variables in a multiple regression equation.
it undermines the statistical significance of an independent variable.
↓
요약
Linearly dependeny variable, span 안에 있는 variable이라고 생각하면 될 듯.

- Use-cases
1. Dimensionality reduction: When your features are highly redundant (multicollinear, specifically), PCA will partition out the redundancy into one or more near-zero variance components, which you can then drop since they will contain little or no information.

2. Anomaly detection: Unusual variation, not apparent from the original features, will often show up in the low-variance components. These components could be highly informative in an anomaly or outlier detection task.

3. Noise reduction: A collection of sensor readings will often share some common background noise. PCA can sometimes collect the (informative) signal into a smaller number of features while leaving the noise alone, thus boosting the signal-to-noise ratio.

4. Decorrelation: Some ML algorithms struggle with highly-correlated features. PCA transforms correlated features into uncorrelated components, which could be easier for your algorithm to work with.

- Things to keep in mink when applying PCA!!
1. PCA only works with numeric features, like continuous quantities or counts.
2. PCA is sensitive to scale. It's good practice to standardize your data before applying PCA, unless you know you have good reason not to.
3. Consider removing or constraining outliers, since they can an have an undue influence on the results.

```python
# Convert to dataframe
component_names = [f"PC{i+1}" for i in range(X_pca.shape[1])]
X_pca = pd.DataFrame(X_pca, columns=component_names)
```
After fitting, the `PCA` instance contains the loadings in its `components_` attribute.

- use the results of PCA to discover one or more new features that improve the performance of your model. by
1. creating features inspired by the loadings
2. using the components themselves as features
(add one or more columns of `X_pca` to `X`)

neither small houses nor houses with large basements are unusual, but it is unusual for small houses to have large basements.
That's the kind of thing a principal component can show you.

-----------------------------------------------
Target Encoding: for categorical features
- encoding with the difference that it also uses the target to create the encoding.
- supervised feature engineering technique.

Mean Encoding: A simple and effective: apply a group aggregation from Lesson 3
Bin Counting: if applied to a binary target
(likelihood encoding, impact encoding, and leave-one-out encoding...)

Problem of Mean Encoding
1. Unknown Categories
target encoding create a special risk of overfitting, which means they need to be trained on an independent "encoding" split.
When you join the encoding to future splits, Pandas will fill in missing values for any categories not present in the encoding split. These missing values you would have to impute somehow.
2. Rare Categories
Target encoding rare categories can make overfitting more likely. Dataset에서 한번밖에 나오지 않았는데 평균을 계산하니까.

Solution >> add `smoothing`
blend the in-category average with the overall average.
Rare categories get less weight on their category average, while missing categories just get the overall average.
```
encoding = weight * in_category + (1 - weight) * overall
```
`weight` is a value between 0 and 1 calculated from the category frequency.
- m-estimate
```
weight = n / (n + m)
```
`n` :total number of times that category occurs in the data. The `m` determines the "smoothing factor". Larger `m` put more weight on the overall estimate.

When choosing a value for `m`, consider how noisy you expect the categories to be.
ex) Does the price of a vehicle vary a great deal within each make? Would you need a lot of data to get good estimates? If so, larger`m` will be better

- Use Cases for Target Encoding
1. High-cardinality features (large number of categories)
One-hot encoding would generate too many features and alternatives, like a label encoding, might not be appropriate for that feature.
A target encoding derives numbers for the categories using the feature's most important property: its relationship with the target.

2. Domain-motivated features
From prior experience, you might suspect that a categorical feature should be important even if it scored poorly with a feature metric. A target encoding can help reveal a feature's true informativeness.

```python
# Encoding split
X_encode = df.sample(frac=0.20, random_state=0)
y_encode = X_encode.pop("SalePrice")

# Training split
X_pretrain = df.drop(X_encode.index)
y_train = X_pretrain.pop("SalePrice")```

```python
# Number of unique categories
df.select_dtypes(["object"]).nunique()

# Use value_counts to check for rate categories.
```

Depending on features, you may have ended up with a score significantly worse than the baseline.
In that case, it's likely the extra information gained by the encoding couldn't make up for the loss of data used for the encoding.

- If no encoding split in a no-duplicate category feature?
Since Count never has any duplicate values, the mean-encoded Count is essentially an exact copy of the target. In other words, mean-encoding turned a completely meaningless feature into a perfect feature.

Now, the only reason this worked is because we trained XGBoost on the same set we used to train the encoder. If we had used a hold-out set instead, none of this "fake" encoding would have transferred to the training data.

The lesson is that when using a target encoder it's very important to use separate data sets for training the encoder and training the model. Otherwise the results can be very disappointing!

References
『The Art of Feature Engineering』
a book by Pablo Duboue. An Empirical Analysis of Feature Engineering for Predictive Modeling, an article by Jeff Heaton.

『Feature Engineering for Machine Learning』
a book by Alice Zheng and Amanda Casari. The tutorial on clustering was inspired by this excellent book.

『Feature Engineering and Selection』
a book by Max Kuhn and Kjell Johnson.